[{"content":"Dynamic configuration from control plane There are already some production ready control planes available like\nGloo Istio We are going to experiment with what\u0026rsquo;s at the core of these service meshes - Go control plane\nIn order to tell envoy where to look for the control plane for getting the configuration we need some of the following things:\nnode dynamic_resources static_resources node The node configuration remains the same as in the previous setup of the envoy configurations:\n1 2 3 node: cluster: test-cluster id: test-id dynamic_resources This tells envoy which configurations to update dynamically\n1 2 3 4 5 6 7 8 9 10 11 12 13 dynamic_resources: ads_config: api_type: GRPC transport_api_version: V3 grpc_services: envoy_grpc: cluster_name: xds_cluster cds_config: resource_api_version: V3 ads: {} lds_config: resource_api_version: V3 ads: {} This basically tells envoy that the Cluster Discovery Service and the Listener Discovery Service are being served as part of the ADS (Aggregate Discovery Service). The ads configuration itself is a GRPC api served by a cluster named xds_cluster.\nstatic_resources This will tell envoy where to receive its dynamic configuration from, i.e. where to find the control plane\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 static_resources: clusters: - type: STRICT_DNS typed_extension_protocol_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions explicit_http_config: http2_protocol_options: {} name: xds_cluster http2_protocol_options: {} connect_timeout: 3s load_assignment: cluster_name: xds_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: address: localhost port_value: 18000 Here we define the details of where can envoy find the previously specified xds_cluster serving the ADS configuration.\nThis is the basic configuration that we need to bootstrap our envoy instance to connect to our control plane and start getting updated configuration from it. The main advantage of this pattern over the previously described ones is that we can now take advantage of the powerful programmable API of the control plane and update the configuration programmatically. So lets see ho we can get this done.\nControl Plane At the heart of the control place configuration, there\u0026rsquo;s what envoy calls a Snapshot. Going through the envoy docs, this is what the definition of a snapshot says it is:\nSnapshot is an internally consistent snapshot of xDS resources\nAnd what does the Snapshot look like you may ask?\n1 2 3 4 5 6 7 8 9 type Snapshot struct { Resources [types.UnknownType]Resources // VersionMap holds the current hash map of all resources in the snapshot. // This field should remain nil until it is used, at which point should be // instantiated by calling ConstructVersionMap(). // VersionMap is only to be used with delta xDS. VersionMap map[string]map[string]string } Hmm, that isn\u0026rsquo;t very helpful. It doesn\u0026rsquo;t give us much information about what does the Resources hold. On further examiniation of the docs, I found the following method in the cache package:\n1 func NewSnapshot(version string, resources map[resource.Type][]types.Resource) (Snapshot, error) And examining the resource package we get these constants\n1 2 3 4 5 6 7 8 9 10 11 12 13 const ( EndpointType = apiTypePrefix + \u0026#34;envoy.config.endpoint.v3.ClusterLoadAssignment\u0026#34; ClusterType = apiTypePrefix + \u0026#34;envoy.config.cluster.v3.Cluster\u0026#34; RouteType = apiTypePrefix + \u0026#34;envoy.config.route.v3.RouteConfiguration\u0026#34; ScopedRouteType = apiTypePrefix + \u0026#34;envoy.config.route.v3.ScopedRouteConfiguration\u0026#34; ListenerType = apiTypePrefix + \u0026#34;envoy.config.listener.v3.Listener\u0026#34; SecretType = apiTypePrefix + \u0026#34;envoy.extensions.transport_sockets.tls.v3.Secret\u0026#34; ExtensionConfigType = apiTypePrefix + \u0026#34;envoy.config.core.v3.TypedExtensionConfig\u0026#34; RuntimeType = apiTypePrefix + \u0026#34;envoy.service.runtime.v3.Runtime\u0026#34; // AnyType is used only by ADS AnyType = \u0026#34;\u0026#34; ) So now it makes sense – the snapshot is just a fancy register that will store a point-in-time snapshot of the envoy configuration for different envoy resources like listener, route, endpoint etc. This is good but is there anything better? – a better abstraction that we can probably use? It turns out there is. On further examination I found the SnapshotCache :\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type SnapshotCache interface { Cache SetSnapshot(ctx context.Context, node string, snapshot Snapshot) error GetSnapshot(node string) (Snapshot, error) ClearSnapshot(node string) GetStatusInfo(string) StatusInfo GetStatusKeys() []string } func NewSnapshotCache(ads bool, hash NodeHash, logger log.Logger) SnapshotCache SnapshotCache is a snapshot-based cache that maintains a single versioned snapshot of responses per node. SnapshotCache consistently replies with the latest snapshot\nAlright now we have some idea of the structure we want to put around this thing:\nWe will start implementing the same in the following snippets:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func GenerateSnapshot(listenerName, routeName, clusterName, upstreamHost string, listenerPort, upstreamPort uint32) cache.Snapshot { return cache.NewSnapshot(\u0026#34;1\u0026#34;, // endpoints []types.Resource{}, // clusters []types.Resource{makeCluster(clusterName, upstreamHost, upstreamPort)}, // routes []types.Resource{makeRoute(routeName, clusterName, upstreamHost)}, // listeners []types.Resource{makeListener(listenerName, routeName, listenerPort)}, // runtimes []types.Resource{}, // secrets []types.Resource{}, ) } Now we implement the methods makeCluster, makeRoute, makeListener:\nCluster and Endpoint 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import ( \u0026#34;time\u0026#34; cluster \u0026#34;github.com/envoyproxy/go-control-plane/envoy/config/cluster/v3\u0026#34; \u0026#34;github.com/golang/protobuf/ptypes\u0026#34; core \u0026#34;github.com/envoyproxy/go-control-plane/envoy/config/core/v3\u0026#34; endpoint \u0026#34;github.com/envoyproxy/go-control-plane/envoy/config/endpoint/v3\u0026#34; ) func makeCluster(clusterName, upstreamHost string, upstreamPort uint32) *cluster.Cluster { return \u0026amp;cluster.Cluster{ Name: clusterName, ConnectTimeout: ptypes.DurationProto(5 * time.Second), ClusterDiscoveryType: \u0026amp;cluster.Cluster_Type{ Type: cluster.Cluster_LOGICAL_DNS, }, LbPolicy: cluster.Cluster_ROUND_ROBIN, LoadAssignment: makeEndpoint(clusterName, upstreamHost, upstreamPort), DnsLookupFamily: cluster.Cluster_V4_ONLY, } } func makeEndpoint(clusterName, upstreamHost string, upstreamPort uint32) *endpoint.ClusterLoadAssignment { return \u0026amp;endpoint.ClusterLoadAssignment{ ClusterName: clusterName, Endpoints: []*endpoint.LocalityLbEndpoints{ { LbEndpoints: []*endpoint.LbEndpoint{ { HostIdentifier: \u0026amp;endpoint.LbEndpoint_Endpoint{ Endpoint: \u0026amp;endpoint.Endpoint{ Address: \u0026amp;core.Address{ Address: \u0026amp;core.Address_SocketAddress{ SocketAddress: \u0026amp;core.SocketAddress{ Protocol: core.SocketAddress_TCP, Address: upstreamHost, PortSpecifier: \u0026amp;core.SocketAddress_PortValue{ PortValue: upstreamPort, }, }, }, }, }, }, }, }, }, }, } } Route 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func makeRoute(routeName, clusterName, upstreamHost string) *route.RouteConfiguration { return \u0026amp;route.RouteConfiguration{ Name: routeName, VirtualHosts: []*route.VirtualHost{ { Name: \u0026#34;local_service\u0026#34;, Domains: []string{\u0026#34;*\u0026#34;}, Routes: []*route.Route{ { Match: \u0026amp;route.RouteMatch{ PathSpecifier: \u0026amp;route.RouteMatch_Prefix{ Prefix: \u0026#34;/\u0026#34;, }, }, Action: \u0026amp;route.Route_Route{ Route: \u0026amp;route.RouteAction{ ClusterSpecifier: \u0026amp;route.RouteAction_Cluster{ Cluster: clusterName, }, HostRewriteSpecifier: \u0026amp;route.RouteAction_HostRewriteLiteral{ HostRewriteLiteral: upstreamHost, }, }, }, }, }, }, }, } } Listener, connection manager and config source This is probably the most complicated one – but will conform to the structure we defined in the previous article\u0026rsquo;s dynamic listerner config – the only difference being it was in yaml and this is in pure go.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 import ( core \u0026#34;github.com/envoyproxy/go-control-plane/envoy/config/core/v3\u0026#34; listener \u0026#34;github.com/envoyproxy/go-control-plane/envoy/config/listener/v3\u0026#34; hcm \u0026#34;github.com/envoyproxy/go-control-plane/envoy/extensions/filters/network/http_connection_manager/v3\u0026#34; \u0026#34;github.com/envoyproxy/go-control-plane/pkg/cache/types\u0026#34; \u0026#34;github.com/envoyproxy/go-control-plane/pkg/cache/v3\u0026#34; \u0026#34;github.com/envoyproxy/go-control-plane/pkg/resource/v3\u0026#34; \u0026#34;github.com/envoyproxy/go-control-plane/pkg/wellknown\u0026#34; \u0026#34;github.com/golang/protobuf/ptypes\u0026#34; ) func makeListener(listenerName, routeName string, listenerPort uint32) *listener.Listener { manager := makeHTTPConnManager(routeName) typedConfig, err := ptypes.MarshalAny(manager) if err != nil { panic(err) } return \u0026amp;listener.Listener{ Name: listenerName, Address: \u0026amp;core.Address{ Address: \u0026amp;core.Address_SocketAddress{ SocketAddress: \u0026amp;core.SocketAddress{ Protocol: core.SocketAddress_TCP, Address: \u0026#34;0.0.0.0\u0026#34;, PortSpecifier: \u0026amp;core.SocketAddress_PortValue{ PortValue: listenerPort, }, }, }, }, FilterChains: []*listener.FilterChain{ { Filters: []*listener.Filter{ { Name: wellknown.HTTPConnectionManager, ConfigType: \u0026amp;listener.Filter_TypedConfig{ TypedConfig: typedConfig, }, }, }, }, }, } } func makeHTTPConnManager(routeName string) *hcm.HttpConnectionManager { return \u0026amp;hcm.HttpConnectionManager{ CodecType: hcm.HttpConnectionManager_AUTO, StatPrefix: \u0026#34;http\u0026#34;, // AccessLog: []*accesslog.AccessLog{ // { // Name: \u0026#34;envoy.access_loggers.stdout\u0026#34;, // ConfigType: \u0026amp;accesslog.AccessLog_TypedConfig{}, // }, // }, RouteSpecifier: \u0026amp;hcm.HttpConnectionManager_Rds{ Rds: \u0026amp;hcm.Rds{ ConfigSource: makeConfigSource(), RouteConfigName: routeName, }, }, HttpFilters: []*hcm.HttpFilter{ { Name: wellknown.Router, }, }, } } func makeConfigSource() *core.ConfigSource { source := \u0026amp;core.ConfigSource{} source.ResourceApiVersion = resource.DefaultAPIVersion source.ConfigSourceSpecifier = \u0026amp;core.ConfigSource_ApiConfigSource{ ApiConfigSource: \u0026amp;core.ApiConfigSource{ TransportApiVersion: resource.DefaultAPIVersion, ApiType: core.ApiConfigSource_GRPC, SetNodeOnFirstMessageOnly: true, GrpcServices: []*core.GrpcService{ { TargetSpecifier: \u0026amp;core.GrpcService_EnvoyGrpc_{ EnvoyGrpc: \u0026amp;core.GrpcService_EnvoyGrpc{ ClusterName: \u0026#34;xds_cluster\u0026#34;, }, }, }, }, }, } return source } So far what we\u0026rsquo;ve managed to do is to:\nDefine our whole configuration in Go using the envoy\u0026rsquo;s go-control-plane library Wrap that configuration in an envoy Snapshot With one more call we can wrap this Snapshot inside a SnapshotCache:\n1 2 3 4 5 6 snapshotCache = cache.NewSnapshotCache(false, cache.IDHash{}, log) snapshot := snapshot.GenerateSnapshot(\u0026#34;listener_0\u0026#34;, \u0026#34;local_route\u0026#34;, \u0026#34;example_cluster\u0026#34;, \u0026#34;www.envoyproxy.io\u0026#34;, 10000, 80) if err := snapshotCache.SetSnapshot(\u0026#34;test-id\u0026#34;, snapshot); err != nil { log.Errorf(\u0026#34;snapshot error %q for %+v\u0026#34;, err, snapshot) os.Exit(1) } This is pretty simple – with all that we\u0026rsquo;ve already written above in our makeCluster, makeRoute, makeListener methods, we are basically defining:\nA listener name listener_0 A route named local_route A cluster named example_cluster An upstream host which is www.envoyproxy.io Listener port 10000 Upstream port 80 This should mean that this configuration should create a listener binding to port 10000 and proxying to www.envoyproxy.io on port 80\nThe last line is the method SetSnapshot where we tell the SnapshotCache of the active snapshot in the cache and the name of that snapshot.\nSetting up the gRPC services Since in this post we are talking about dynamic serving of envoy through the xDS control plane, the gRPC protocol forms the basis of how this configuration will be served to the envoy proxies. So lets set that up now:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; clusterservice \u0026#34;github.com/envoyproxy/go-control-plane/envoy/service/cluster/v3\u0026#34; discoverygrpc \u0026#34;github.com/envoyproxy/go-control-plane/envoy/service/discovery/v3\u0026#34; endpointservice \u0026#34;github.com/envoyproxy/go-control-plane/envoy/service/endpoint/v3\u0026#34; listenerservice \u0026#34;github.com/envoyproxy/go-control-plane/envoy/service/listener/v3\u0026#34; routeservice \u0026#34;github.com/envoyproxy/go-control-plane/envoy/service/route/v3\u0026#34; runtimeservice \u0026#34;github.com/envoyproxy/go-control-plane/envoy/service/runtime/v3\u0026#34; secretservice \u0026#34;github.com/envoyproxy/go-control-plane/envoy/service/secret/v3\u0026#34; generator \u0026#34;github.com/ishankhare07/envoy-dynamic/pkg/snapshot\u0026#34; \u0026#34;github.com/ishankhare07/envoy-dynamic/pkg/logger\u0026#34; \u0026#34;github.com/envoyproxy/go-control-plane/pkg/cache/v3\u0026#34; \u0026#34;github.com/envoyproxy/go-control-plane/pkg/server/v3\u0026#34; \u0026#34;github.com/envoyproxy/go-control-plane/pkg/test/v3\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/reflection\u0026#34; ) var log *logger.Logger var snapshotCache cache.SnapshotCache func init() { log = \u0026amp;logger.Logger{Debug: true} } func RunServer(ctx context.Context, port uint) { grpcServer := grpc.NewServer() lis, err := net.Listen(\u0026#34;tcp\u0026#34;, fmt.Sprintf(\u0026#34;:%d\u0026#34;, port)) if err != nil { log.Errorf(\u0026#34;%v\u0026#34;, err) } snapshotCache = cache.NewSnapshotCache(false, cache.IDHash{}, log) snapshot := generator.GenerateSnapshot(\u0026#34;listener_0\u0026#34;, \u0026#34;local_route\u0026#34;, \u0026#34;example_cluster\u0026#34;, \u0026#34;www.envoyproxy.io\u0026#34;, 10000, 80) if err := snapshot.Consistent(); err != nil { log.Errorf(\u0026#34;snapshot inconsistency: %+v\\n%+v\u0026#34;, snapshot, err) os.Exit(1) } t, _ := json.MarshalIndent(snapshot, \u0026#34;\u0026#34;, \u0026#34; \u0026#34;) log.Debugf(\u0026#34;will serve snapshot %s\u0026#34;, t) // Add the snapshot to the cache if err := snapshotCache.SetSnapshot(\u0026#34;test-id\u0026#34;, snapshot); err != nil { log.Errorf(\u0026#34;snapshot error %q for %+v\u0026#34;, err, snapshot) os.Exit(1) } cb := \u0026amp;test.Callbacks{Debug: log.Debug} srv := server.NewServer(ctx, snapshotCache, cb) registerServer(grpcServer, srv) helloworldservice.RegisterGreeterServer(grpcServer, helloworldservice.NewHelloWorldServer(snapshotCache)) reflection.Register(grpcServer) log.Infof(\u0026#34;management server listening on port %d\\n\u0026#34;, port) if err = grpcServer.Serve(lis); err != nil { log.Errorf(\u0026#34;%v\u0026#34;, err) panic(err) } } func registerServer(grpcServer *grpc.Server, server server.Server) { // register services discoverygrpc.RegisterAggregatedDiscoveryServiceServer(grpcServer, server) endpointservice.RegisterEndpointDiscoveryServiceServer(grpcServer, server) clusterservice.RegisterClusterDiscoveryServiceServer(grpcServer, server) routeservice.RegisterRouteDiscoveryServiceServer(grpcServer, server) listenerservice.RegisterListenerDiscoveryServiceServer(grpcServer, server) secretservice.RegisterSecretDiscoveryServiceServer(grpcServer, server) runtimeservice.RegisterRuntimeDiscoveryServiceServer(grpcServer, server) } So our previously posted diagram becomes something like this\nLets make sure we serve this!\n1 2 3 4 5 6 7 8 9 10 11 12 package main import ( \u0026#34;context\u0026#34; aggregateGRPCServer \u0026#34;github.com/ishankhare07/envoy-dynamic/pkg/server\u0026#34; ) func main() { ctx := context.Background() aggregateGRPCServer.RunServer(ctx, 18000) } So our xDS server will be serving configuration on port 18000, and according to the configuration listener_0 will be binding and proxying on localhost:10000 to www.envoyproxy.io:80\nRunning the envoy proxy Since we are serving the config from xDS control plane this time, does that mean we don\u0026rsquo;t need to write any static_configuration now? Unfortunately NO! – We still need to tell envoy where the dynamic configuration is being server at. The main flexibiliy that this control plane mode is allowing us is to programatically control the configuration – something that we\u0026rsquo;ll explore soon in the following blog posts of this series. but for now we still need to write some basic envoy config: envoy-dynamic-control-plane.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 node: cluster: test-cluster id: test-id dynamic_resources: ads_config: api_type: GRPC transport_api_version: V3 grpc_services: envoy_grpc: cluster_name: xds_cluster cds_config: resource_api_version: V3 ads: {} lds_config: resource_api_version: V3 ads: {} static_resources: clusters: - type: STRICT_DNS typed_extension_protocol_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions explicit_http_config: http2_protocol_options: {} name: xds_cluster http2_protocol_options: {} connect_timeout: 3s load_assignment: cluster_name: xds_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: address: localhost port_value: 18000 admin: access_log_path: /dev/null address: socket_address: address: 0.0.0.0 port_value: 19000 layered_runtime: layers: - name: runtime-0 rtds_layer: rtds_config: resource_api_version: V3 api_config_source: transport_api_version: V3 api_type: GRPC grpc_services: envoy_grpc: cluster_name: xds_cluster name: runtime-0 Interesting parts are:\ndynamic_resources.ads_config – here we tell envoy to: Expect a GRPC configuration Conforming to V3 of the envoy api Served by a envoy cluster named xds_cluster – where would envoy find xds_cluster now? Read along static_resources.clusters[0].name – answer to your question above static_resources.clusters[0].endpoints – what endpoints to expect in this cluster (basically the host and port of our gRPC xDS server serving the dynamic configuration) Here\u0026rsquo;s what things look like now\nRunning everything If you\u0026rsquo;ve followed along till now, here\u0026rsquo;s the reward. Lets run the control plane and the envoy server:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 # in one shell let\u0026#39;s start the control plane go run main.go 2022/01/14 03:11:48 will serve snapshot { \u0026#34;Resources\u0026#34;: [ { \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Items\u0026#34;: {} }, { \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Items\u0026#34;: { \u0026#34;example_cluster\u0026#34;: { \u0026#34;Resource\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;example_cluster\u0026#34;, \u0026#34;ClusterDiscoveryType\u0026#34;: { \u0026#34;Type\u0026#34;: 2 }, \u0026#34;connect_timeout\u0026#34;: { \u0026#34;seconds\u0026#34;: 5 }, \u0026#34;load_assignment\u0026#34;: { \u0026#34;cluster_name\u0026#34;: \u0026#34;example_cluster\u0026#34;, \u0026#34;endpoints\u0026#34;: [ { \u0026#34;lb_endpoints\u0026#34;: [ { \u0026#34;HostIdentifier\u0026#34;: { \u0026#34;Endpoint\u0026#34;: { \u0026#34;address\u0026#34;: { \u0026#34;Address\u0026#34;: { \u0026#34;SocketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;www.envoyproxy.io\u0026#34;, \u0026#34;PortSpecifier\u0026#34;: { \u0026#34;PortValue\u0026#34;: 80 } } } } } } } ] } ] }, \u0026#34;dns_lookup_family\u0026#34;: 1, \u0026#34;LbConfig\u0026#34;: null }, \u0026#34;Ttl\u0026#34;: null } } }, { \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Items\u0026#34;: { \u0026#34;local_route\u0026#34;: { \u0026#34;Resource\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;local_route\u0026#34;, \u0026#34;virtual_hosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;local_service\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;routes\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;PathSpecifier\u0026#34;: { \u0026#34;Prefix\u0026#34;: \u0026#34;/\u0026#34; } }, \u0026#34;Action\u0026#34;: { \u0026#34;Route\u0026#34;: { \u0026#34;ClusterSpecifier\u0026#34;: { \u0026#34;Cluster\u0026#34;: \u0026#34;example_cluster\u0026#34; }, \u0026#34;HostRewriteSpecifier\u0026#34;: { \u0026#34;HostRewriteLiteral\u0026#34;: \u0026#34;www.envoyproxy.io\u0026#34; } } } } ] } ] }, \u0026#34;Ttl\u0026#34;: null } } }, { \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Items\u0026#34;: { \u0026#34;listener_0\u0026#34;: { \u0026#34;Resource\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;listener_0\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;Address\u0026#34;: { \u0026#34;SocketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;PortSpecifier\u0026#34;: { \u0026#34;PortValue\u0026#34;: 10000 } } } }, \u0026#34;filter_chains\u0026#34;: [ { \u0026#34;filters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;, \u0026#34;ConfigType\u0026#34;: { \u0026#34;TypedConfig\u0026#34;: { \u0026#34;type_url\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;EgRodHRwKhsKGWVudm95LmZpbHRlcnMuaHR0cC5yb3V0ZXIaKgobMAISFwgCIg8KDQoLeGRzX2NsdXN0ZXI4AUACEgtsb2NhbF9yb3V0ZQ==\u0026#34; } } } ] } ], \u0026#34;ListenerSpecifier\u0026#34;: null }, \u0026#34;Ttl\u0026#34;: null } } }, { \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Items\u0026#34;: {} }, { \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Items\u0026#34;: {} }, { \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Items\u0026#34;: {} } ], \u0026#34;VersionMap\u0026#34;: null } 2022/01/14 03:11:48 management server listening on port 18000 We are spitting out the served xDS configuration for convenience. Lets start the envoy server in another terminal with the previously defined configuration:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 envoy -c envoy-dynamic-control-plane.yaml -l debug . . . [2022-01-14 03:14:28.280][4602226][debug][router] [source/common/router/router.cc:486] [C0][S9032965470346210647] cluster \u0026#39;xds_cluster\u0026#39; match for URL \u0026#39;/envoy.service.discovery.v3.AggregatedDiscoveryServic e/StreamAggregatedResources\u0026#39; [2022-01-14 03:14:28.280][4602226][debug][router] [source/common/router/router.cc:702] [C0][S9032965470346210647] router decoding headers: \u0026#39;:method\u0026#39;, \u0026#39;POST\u0026#39; \u0026#39;:path\u0026#39;, \u0026#39;/envoy.service.discovery.v3.AggregatedDiscoveryService/StreamAggregatedResources\u0026#39; \u0026#39;:authority\u0026#39;, \u0026#39;xds_cluster\u0026#39; \u0026#39;:scheme\u0026#39;, \u0026#39;http\u0026#39; \u0026#39;te\u0026#39;, \u0026#39;trailers\u0026#39; \u0026#39;content-type\u0026#39;, \u0026#39;application/grpc\u0026#39; \u0026#39;x-envoy-internal\u0026#39;, \u0026#39;true\u0026#39; \u0026#39;x-forwarded-for\u0026#39;, \u0026#39;192.168.29.160\u0026#39; . . . . There will be lots of information but the one I\u0026rsquo;ve pasted above confirms that envoy was able to find the xds_cluster serving our configuration. Also if we go and look at the control_plane terminal, we\u0026rsquo;ll find this:\n1 2 3 4 5 6 7 8 9 10 11 2022/01/14 03:14:27 stream 1 open for type.googleapis.com/envoy.service.runtime.v3.Runtime 2022/01/14 03:14:27 respond type.googleapis.com/envoy.service.runtime.v3.Runtime[runtime-0] version \u0026#34;\u0026#34; with version \u0026#34;1\u0026#34; 2022/01/14 03:14:27 open watch 1 for type.googleapis.com/envoy.service.runtime.v3.Runtime[runtime-0] from nodeID \u0026#34;test-id\u0026#34;, version \u0026#34;1\u0026#34; 2022/01/14 03:14:28 stream 2 open for 2022/01/14 03:14:42 respond type.googleapis.com/envoy.config.cluster.v3.Cluster[] version \u0026#34;\u0026#34; with version \u0026#34;1\u0026#34; 2022/01/14 03:14:42 open watch 2 for type.googleapis.com/envoy.config.cluster.v3.Cluster[] from nodeID \u0026#34;test-id\u0026#34;, version \u0026#34;1\u0026#34; 2022/01/14 03:14:43 respond type.googleapis.com/envoy.config.listener.v3.Listener[] version \u0026#34;\u0026#34; with version \u0026#34;1\u0026#34; 2022/01/14 03:14:43 stream 3 open for type.googleapis.com/envoy.config.route.v3.RouteConfiguration 2022/01/14 03:14:43 open watch 3 for type.googleapis.com/envoy.config.listener.v3.Listener[] from nodeID \u0026#34;test-id\u0026#34;, version \u0026#34;1\u0026#34; 2022/01/14 03:14:43 respond type.googleapis.com/envoy.config.route.v3.RouteConfiguration[local_route] version \u0026#34;\u0026#34; with version \u0026#34;1\u0026#34; 2022/01/14 03:14:43 open watch 4 for type.googleapis.com/envoy.config.route.v3.RouteConfiguration[local_route] from nodeID \u0026#34;test-id\u0026#34;, version \u0026#34;1\u0026#34; Let\u0026rsquo;s test by visiting http://localhost:10000 – on clicking this link you should be redirected to www.envoyproxy.io:80\nAnd there you have it!\nIf you would like to see the whole repo and code better organized that this blog post, head ove to this github repo – github.com/ishankhare07/envoy-dynamic\nWhat next?! Continuing my habbit of fiddling with all kinds of things, I\u0026rsquo;ve decided to convert this already long 2 part series of posts even longer. The goal is to start from envoy basics already discussed in the past and the current post and going all the way up to a very curde service mesh implementation. In the coming days I would be covering more of the following topics:\nDynamically adding more upstreams to our control plane through an exposed gRPC endpoint. Write a sidecar injector that is able to inject envoy proxies into our k8s pods Serve control plane inside k8s cluster with the sidecar injector enabled Make the envoy sidecars fetch the dynamic configuration from the in-cluster control plane and proxy few upstreams. Wrap the control plane in a k8s controller and use a CRD to add / remove upstreams Proxy Pod-to-Pod communication and updating the upstream configuration using the k8s controller. ","date":"2022-01-14T00:00:00Z","image":"https://ishankhare.dev/p/dabbling-with-envoy-configurations-part-ii/PXL_20211114_102320720_hu082ecdbb90ad1c64e25ff84bfc0ff1a8_2983891_120x120_fill_q75_box_smart1.jpg","permalink":"https://ishankhare.dev/p/dabbling-with-envoy-configurations-part-ii/","title":"Dabbling with envoy - Part II"},{"content":"Types of envoy configurations Static configuration\nDynamic configuration. This can be done in two ways:\nFrom Filesystem From Control plane Static configuration To start envoy in static configuration we need the following:\nlisteners clusters static_reources (Optional) admin section static_resources Contain everything that is configured statically when envoy starts. Can contain the following:\n[]listeners []clusters []secrets listeners Lets configure an example listener on port 10000. Here all paths are matched and routed to service_envoyproxy_io cluster\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 listeners: - name: listener_0 address: socket_address: address: 0.0.0.0 port_value: 10000 filter_chains: - filters: - name: envoy.filters.network.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http access_log: - name: envoy.access_loggers.stdout typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog http_filters: - name: envoy.filters.http.router route_config: name: local_route virtual_hosts: - name: local_service domains: [\u0026#34;*\u0026#34;] routes: - match: prefix: \u0026#34;/\u0026#34; route: host_rewrite_literal: www.envoyproxy.io cluster: service_envoyproxy_io cluster The service_envoyproxy_io cluster proxies over TLS to https://www.envoyproxy.io\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 clusters: - name: service_envoyproxy_io type: LOGICAL_DNS # Comment out the following line to test on v6 networks dns_lookup_family: V4_ONLY load_assignment: cluster_name: service_envoyproxy_io endpoints: - lb_endpoints: - endpoint: address: socket_address: address: www.envoyproxy.io port_value: 443 transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext sni: www.envoyproxy.io Testing this static configuration If now we start envoy with this configuration using command envoy -c \u0026lt;config_name\u0026gt;.yaml and try querying the localhost:10000 port, we should get the envoyproxy homepage.\n1 curl -v localhost:10000 Dynamic Configuration from filesystem In this setup Envoy will automatically update its configuration whenever the files are changed on the filesystem. The following sections are a must for dynamic configuration:\nnode dynamic_resources node node needs a cluster and an id\n1 2 3 node: cluster: test-cluster id: test-id dynamic_resources Specifies where to load dynamic configuration from\n1 2 3 4 5 dynamic_resources: cds_config: path: ./cds.yaml lds_config: path: ./lds_yaml listener resources The linked lds_config should be an implementation of a Listener Discovery Service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 resources: - \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.listener.v3.Listener name: listener_0 address: socket_address: address: 0.0.0.0 port_value: 10000 filter_chains: - filters: - name: envoy.http_connection_manager typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager stat_prefix: ingress_http http_filters: - name: envoy.router route_config: name: local_route virtual_hosts: - name: local_service domains: - \u0026#34;*\u0026#34; routes: - match: prefix: \u0026#34;/\u0026#34; route: host_rewrite_literal: www.envoyproxy.io cluster: example_proxy_cluster cluster resources The linked cds_config should be an implementation of a Cluster Discovery Service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 resources: - \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Cluster name: example_proxy_cluster type: STRICT_DNS connect_timeout: 3s typed_extension_protocol_options: envoy.extensions.upstreams.http.v3.HttpProtocolOptions: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions explicit_http_config: http2_protocol_options: {} load_assignment: cluster_name: example_proxy_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: address: www.envoyproxy.io port_value: 443 transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext sni: www.envoyproxy.io Dynamically editing the configuration Let\u0026rsquo;s try editing this config to start proxying to google.com instead of envoyproxy.io\nIn the lds.yaml file change the following:\n1 2 3 4 5 6 7 routes: - match: prefix: \u0026#34;/\u0026#34; route: - host_rewrite_literal: www.envoyproxy.io + host_rewrite_literal: www.google.com cluster: example_proxy_cluster As soon as we do this write in the file, the LDS config in the envoy will update and will show in the logs:\n1 lds: add/update listener \u0026#39;listener_0\u0026#39; We need to update the cds.yaml config as well:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 load_assignment: cluster_name: example_proxy_cluster endpoints: - lb_endpoints: - endpoint: address: socket_address: - address: www.envoyproxy.io + address: www.google.com port_value: 443 transport_socket: name: envoy.transport_sockets.tls typed_config: \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext - sni: www.envoyproxy.io + sni: www.google.com We should see the similar update in envoy\u0026rsquo;s logs about the CDS config update\n1 cds: added/updated 1 cluster(s), skipped 0 unmodified cluster(s) Hence we were able to reload the envoy configuration dynamically without restarting the server itself.\n","date":"2021-06-28T00:00:00Z","image":"https://ishankhare.dev/p/dabbling-with-envoy-configurations-part-i/cover_huca0165527005b89d1a6b5500db0bdef1_3693180_120x120_fill_q75_box_smart1.jpg","permalink":"https://ishankhare.dev/p/dabbling-with-envoy-configurations-part-i/","title":"Dabbling with Envoy configurations - Part I"},{"content":"What we\u0026rsquo;ll cover in this post? This post in general is about how we can leverage kubernetes controllers and the kubernetes resource model to automate and manage heavy lifting of tricky deployment and management scenarios.\nController are often explained in layman terms as software-SRE\u0026rsquo;s. The good thing about that is:\nThey are always available and ready to step in when things go wrong. They have all the operational logic in the form of code related to the steps to take when certain things don\u0026rsquo;t work. They don\u0026rsquo;t need to go over runbooks/playbooks or other documentations to fix issues. Since they are always watching resources and their states (as we\u0026rsquo;ll see later in this post), they don\u0026rsquo;t have to spend time to find where the errors/misconfiguration happened. This can give us a few positive outcomes:\nHumans don\u0026rsquo;t have to be on hectic on-call schedules. In cases of errors, the software itself is healing the system, hence the MTTR(mean time to response) is considerably decreased. \u0026ldquo;With great power comes great responsibilities\u0026rdquo;. With all the advantages of implementing controllers, they also come with high cost of implementation details. Implementing controllers is generally more complex than traditional systems.\nNow that we know a bit about controllers, we\u0026rsquo;ll take a use case and try to implement a controller to solve it.\nTl;Dr If you want to directly go ahead and checkout the code implementation and test it on your local machine, I have the entire thing residing in this – github.com/ishankhare07/launchpad repo. There is a detailed readme which walks you through a Makefile that:\nSets up a local kind cluster Installs Istio on it Deploys demo workloads(Deployments) Applies the right Istio CRD\u0026rsquo;s – a VirtualService and a DestinationRule to achieve a traffic split. Check and verify the traffic split. It then also walks you through the controller way of doing things and also the magic of Software-SRE\u0026rsquo;s that they bring along.\nUse case The use case we are trying to tackle here is doing a traffic split using a a service mesh. Since this blog post can only have limited scope, what we will NOT talk about is:\nService mesh in general or service mesh comparison. We\u0026rsquo;ll just go ahead and use Istio. We\u0026rsquo;ll not go into the details of how Istio or any other service mesh works. The specific scenario we would want to cover is a Traffic Split/Canary. More details can be found by giving a look at this awesome example site – istiobyexample.dev/canary – by @askmeegs. Borrowing the following screenshot from that same article, demonstrating what we want to achieve:\nThe page also details how we can achieve this by creating Istio specific CRD\u0026rsquo;s. We\u0026rsquo;ll look at both the approaches and see the advantages first-hand where controllers come in.\nBelow are the Istio CRD\u0026rsquo;s that we would want to create in the cluster in order for Istio to do things for us:\nDestinationRule\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: helloworld spec: host: helloworld subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 VirtualService\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld spec: hosts: - helloworld http: - route: - destination: host: helloworld subset: v1 weight: 80 - destination: host: helloworld subset: v2 weight: 20 Too much going around the core of this post, lets dive in now.\nThis post uses the awesome kubebuilder framework for implementing the controller. If you\u0026rsquo;re unfamiliar about it and want an intro I\u0026rsquo;ve written a previous article about it on my blog which you can read here – Writing a kubernetes controller in Go with kubebuilder\nThe Deep Dive We\u0026rsquo;ll start by initializing our project for the controller using go mod and the kubebuilder framework\n1 2 3 4 5 6 7 8 9 10 $ go mod init github.com/ishankhare07/launchpad $ kubebuilder init --domain ishankhare.dev # create scaffolding for the project $ kubebuilder create api --group labs --version v1alpha1 --kind Workload Create Resource [y/n] y Create Controller [y/n] y Writing scaffold for you to edit... The CRD Next step is to define the CRD which we want to use for defining our traffic split to our controller. We want roughly the following yaml spec for it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: labs.ishankhare.dev/v1alpha1 kind: Workload metadata: name: workload-sample spec: host: helloworld targets: - name: helloworld-v1 namespace: default trafficSplit: hosts: - helloworld subset: name: v1 labels: version: v1 destinationHost: helloworld weight: 80 - name: helloworld-v2 namespace: default trafficSplit: hosts: - helloworld subset: name: v2 labels: version: v2 destinationHost: helloworld weight: 20 I know there\u0026rsquo;s room for improvement and this can be refined further, but for the purpose of this post, this should do fine. Lets go about defining this spec in our api/v1alpha1/workload_types.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 ... const ( PhasePending = \u0026#34;PENDING\u0026#34; PhaseCreated = \u0026#34;CREATED\u0026#34; ) // +kubebuilder:object:root=true // +kubebuilder:subresource:status // Workload is the Schema for the workloads API type Workload struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec WorkloadSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status WorkloadStatus `json:\u0026#34;status,omitempty\u0026#34;` } type WorkloadSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file // Host is the name of the service from the service registry Host string `json:\u0026#34;host,omitempty\u0026#34;` Targets []DeploymentTarget `json:\u0026#34;targets,omitempty\u0026#34;` } // WorkloadStatus defines the observed state of Workload type WorkloadStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file VirtualServiceState string `json:\u0026#34;virtualServiceState,omitempty\u0026#34;` DestinationRuleState string `json:\u0026#34;destinationRuleState,omitempty\u0026#34;` } type DeploymentTarget struct { Name string `json:\u0026#34;name,omitempty\u0026#34;` Namespace string `json:\u0026#34;namespace,omitempty\u0026#34;` TrafficSplit TrafficSplit `json:\u0026#34;trafficSplit,omitempty\u0026#34;` } type TrafficSplit struct { Hosts []string `json:\u0026#34;hosts,omitempty\u0026#34;` Subset Subset `json:\u0026#34;subset,omitempty\u0026#34;` DestinationHost string `json:\u0026#34;destinationHost,omitempty\u0026#34;` Weight int `json:\u0026#34;weight,omitempty\u0026#34;` } type Subset struct { Name string `json:\u0026#34;name,omitempty\u0026#34;` Labels map[string]string `json:\u0026#34;labels,omitempty\u0026#34;` } func (w *Workload) GetIstioResourceName() string { return w.Spec.Host // strings.Split(w.Spec.Targets[0].Name, \u0026#34;-\u0026#34;)[0] } func (w *Workload) GetHosts() []string { // need hosts as a set data structure hostSet := make(map[string]bool) hosts := []string{} for _, target := range w.Spec.Targets { for _, host := range target.TrafficSplit.Hosts { hostSet[host] = true } } for host := range hostSet { hosts = append(hosts, host) } return hosts } This should be enough to define our define the yaml that we presented before. If you\u0026rsquo;re following along the code in the repo, the above is defined in api/v1alpha1/workload_types.go. We also add some additional helper methods on the objects that will be useful later.\nThe controller Let\u0026rsquo;s jump to controllers/workload_controller.go. I\u0026rsquo;m only going to show the relevant sections of the code here in favour of brevity. For detailed code one can go and see the linked file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 ... func (r *WorkloadReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { reqLogger := r.Log.WithValues(\u0026#34;workload\u0026#34;, req.NamespacedName) // your logic here reqLogger.Info(\u0026#34;====== Reconciling Workload =======\u0026#34;) instance := \u0026amp;labsv1alpha1.Workload{} err := r.Get(context.TODO(), req.NamespacedName, instance) if err != nil { // object not found, could have been deleted after // reconcile request, hence don\u0026#39;t requeue if errors.IsNotFound(err) { return ctrl.Result{}, nil } // error reading the object, requeue the request return ctrl.Result{}, err } reqLogger.Info(\u0026#34;Workload created for following targets.\u0026#34;) for _, target := range instance.Spec.Targets { reqLogger.Info(\u0026#34;target\u0026#34;, \u0026#34;name\u0026#34;, target.Name, \u0026#34;namespace\u0026#34;, target.Namespace, \u0026#34;hosts\u0026#34;, fmt.Sprintf(\u0026#34;%v\u0026#34;, target.TrafficSplit.Hosts)) reqLogger.Info(\u0026#34;subset\u0026#34;, \u0026#34;subset name\u0026#34;, target.TrafficSplit.Subset.Name, \u0026#34;subset labels\u0026#34;, fmt.Sprintf(\u0026#34;%v\u0026#34;, target.TrafficSplit.Subset.Labels), \u0026#34;weight\u0026#34;, target.TrafficSplit.Weight) } // check current status of owned resources if instance.Status.VirtualServiceState == \u0026#34;\u0026#34; { instance.Status.VirtualServiceState = labsv1alpha1.PhasePending } if instance.Status.DestinationRuleState == \u0026#34;\u0026#34; { instance.Status.DestinationRuleState = labsv1alpha1.PhasePending } // check virtual service status vsResult, err := r.checkVirtualServiceStatus(instance, req, reqLogger) if err != nil { return vsResult, err } // check destination rule status destRuleResult, err := r.checkDestinationRuleStatus(instance, req, reqLogger) if err != nil { return destRuleResult, err } // update status err = r.Status().Update(context.TODO(), instance) if err != nil { return ctrl.Result{}, err } return ctrl.Result{}, nil } Here we are simply logging what the controller \u0026ldquo;sees\u0026rdquo; is requested to be created as \u0026ldquo;desired state\u0026rdquo; through the CRD. Next we are calling the relevant functions checkVirtualServiceStatus and checkDestinationRuleStatus. There implementation is explained below, with just the following intention:\nIf the VirtualService or DestinationRule doesn\u0026rsquo;t exist yet, try to create it with the correct spec.\nThe VirtualService check In the same file let\u0026rsquo;s implement the checkVirtualServiceStatus method, please scroll to the right for extra comments explaining each section:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 ... func (r *WorkloadReconciler) checkVirtualServiceStatus(instance *labsv1alpha1.Workload, req ctrl.Request, logger logr.Logger) (ctrl.Result, error) { switch instance.Status.VirtualServiceState { case labsv1alpha1.PhasePending: logger.Info(\u0026#34;Virtual Service\u0026#34;, \u0026#34;PHASE:\u0026#34;, instance.Status.VirtualServiceState)\t// If still in pending state (initial condition) logger.Info(\u0026#34;Transitioning state to create Virtual Service\u0026#34;)\t// transition to the creating state\tinstance.Status.VirtualServiceState = labsv1alpha1.PhaseCreated\t// case labsv1alpha1.PhaseCreated: logger.Info(\u0026#34;Virtual Service\u0026#34;, \u0026#34;PHASE:\u0026#34;, instance.Status.VirtualServiceState)\t// query := \u0026amp;istiov1alpha3.VirtualService{}\t// create an empty query object // // check if virtual service already exists\t//---------\\ lookupKey := types.NamespacedName{\t//\t\\ Name: instance.GetIstioResourceName(),\t//\t--- construct the query object Namespace: instance.GetNamespace(),\t//\t/ }\t//---------/ err := r.Get(context.TODO(), lookupKey, query)\t// query result if err != nil \u0026amp;\u0026amp; errors.IsNotFound(err) {\t// logger.Info(\u0026#34;virtual service not found but should exist\u0026#34;, \u0026#34;lookup key\u0026#34;, lookupKey)\t// logger.Info(err.Error())\t// // virtual service got deleted or hasn\u0026#39;t been created yet\t// // create one now\t// vs := spawn.CreateVirtualService(instance)\t// create the actual virtual service as defined by Istio err = ctrl.SetControllerReference(instance, vs, r.Scheme)\t// Set owner references on the created object if err != nil {\t// logger.Error(err, \u0026#34;Error setting controller reference\u0026#34;)\t// return ctrl.Result{}, err\t// }\t// // err = r.Create(context.TODO(), vs)\t// request the API server to create the actual object in if err != nil {\t// the cluster i.e. persist in the etcd store logger.Error(err, \u0026#34;Unable to create Virtual Service\u0026#34;)\t// return ctrl.Result{}, err\t// }\t// // logger.Info(\u0026#34;Successfully created virtual service\u0026#34;)\t// } else if err != nil {\t// logger.Error(err, \u0026#34;Unable to create Virtual Service\u0026#34;)\t// return ctrl.Result{}, err\t// } else {\t// // don\u0026#39;t requeue, it will happen automatically when\t// // virtual service status changes\t// return ctrl.Result{}, nil\t// }\t// // more fields related to virtual service status can be checked // see more at https://pkg.go.dev/istio.io/api/meta/v1alpha1#IstioStatus } return ctrl.Result{}, nil } The line vs := spawn.CreateVirtualService(instance) defines another function to create the actual Istio VirtualService for us which we will define further in the spawn package.\nThe DestinationRule check Similar to the virtual service, we\u0026rsquo;ll create the checkDestinationRuleStatus method in the same file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ... func (r *WorkloadReconciler) checkDestinationRuleStatus(instance *labsv1alpha1.Workload, req ctrl.Request, logger logr.Logger) (ctrl.Result, error) { switch instance.Status.DestinationRuleState { case labsv1alpha1.PhasePending: logger.Info(\u0026#34;Destination Rule\u0026#34;, \u0026#34;PHASE:\u0026#34;, instance.Status.DestinationRuleState) logger.Info(\u0026#34;Transitioning state to create Destination Rule\u0026#34;) instance.Status.DestinationRuleState = labsv1alpha1.PhaseCreated case labsv1alpha1.PhaseCreated: logger.Info(\u0026#34;Destination Rule\u0026#34;, \u0026#34;PHASE:\u0026#34;, instance.Status.DestinationRuleState) query := \u0026amp;istiov1alpha3.DestinationRule{} // check if destination rule already exists lookupKey := types.NamespacedName{ Name: instance.GetIstioResourceName(), Namespace: instance.GetNamespace(), } err := r.Get(context.TODO(), lookupKey, query) if err != nil \u0026amp;\u0026amp; errors.IsNotFound(err) { logger.Info(\u0026#34;destination rule not found but should exist\u0026#34;, \u0026#34;req key\u0026#34;, lookupKey) logger.Info(err.Error()) // destination rule got deleted or hasn\u0026#39;t been created yet // create one now dr := spawn.CreateDestinationRule(instance) err := ctrl.SetControllerReference(instance, dr, r.Scheme) if err != nil { logger.Error(err, \u0026#34;Error setting controller reference\u0026#34;) return ctrl.Result{}, err } err = r.Create(context.TODO(), dr) if err != nil { logger.Error(err, \u0026#34;Unable to create Destination Rule\u0026#34;) return ctrl.Result{}, err } logger.Info(\u0026#34;Successfully created Destination Rule\u0026#34;) } else if err != nil { logger.Error(err, \u0026#34;Unable to create destination rule\u0026#34;) return ctrl.Result{}, err } else { // don\u0026#39;t requeue, it will happen automatically when // destination rule status changes return ctrl.Result{}, nil } } return ctrl.Result{}, nil } The line dr := spawn.CreateDestinationRule(instance) defines another function to create the actual Istio DestinationRule for us, which we will define further in the spawn package.\nCreating the Istio CRD\u0026rsquo;s (defining the \u0026lsquo;spawn\u0026rsquo;) package Let\u0026rsquo;s create the following two files in the pkg directory:\ndestination-rule.go virtual-service.go The structure should be as follows:\n1 2 3 4 5 $ tree ./pkg ./pkg └── spawn ├── destination-rule.go └── virtual-service.go In the file virtual-service.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package spawn import ( labsv1alpha1 \u0026#34;github.com/ishankhare07/launchpad/api/v1alpha1\u0026#34; istionetworkingv1alpha3 \u0026#34;istio.io/api/networking/v1alpha3\u0026#34;\t// ---------\\ // - import the required Istio structures (CRD\u0026#39;s) istiov1alpha3 \u0026#34;istio.io/client-go/pkg/apis/networking/v1alpha3\u0026#34;\t// ---------/ metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; ) func CreateVirtualService(workload *labsv1alpha1.Workload) *istiov1alpha3.VirtualService { vs := \u0026amp;istiov1alpha3.VirtualService{ ObjectMeta: metav1.ObjectMeta{ Name: workload.GetIstioResourceName(), Namespace: \u0026#34;default\u0026#34;, }, } routeDestination := []*istionetworkingv1alpha3.HTTPRouteDestination{} for _, target := range workload.Spec.Targets { // extract route from CRD route := \u0026amp;istionetworkingv1alpha3.HTTPRouteDestination{ Destination: \u0026amp;istionetworkingv1alpha3.Destination{ Host: target.TrafficSplit.DestinationHost, Subset: target.TrafficSplit.Subset.Name, }, Weight: int32(target.TrafficSplit.Weight), } // append routes into route destination routeDestination = append(routeDestination, route) } // append route destination to VirtualService.Routes vs.Spec.Http = append(vs.Spec.Http, \u0026amp;istionetworkingv1alpha3.HTTPRoute{ Route: routeDestination, }) // set hosts vs.Spec.Hosts = workload.GetHosts() return vs } We import the right structures from the Istio\u0026rsquo;s upstream repo and construct the object based on the values provided in our original Workload CRD. In the similar way we define the destination rule object generation in the file destination-rule.go\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package spawn import ( labsv1alpha1 \u0026#34;github.com/ishankhare07/launchpad/api/v1alpha1\u0026#34; istionetworkingv1alpha3 \u0026#34;istio.io/api/networking/v1alpha3\u0026#34; istiov1alpha3 \u0026#34;istio.io/client-go/pkg/apis/networking/v1alpha3\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; ) func CreateDestinationRule(workload *labsv1alpha1.Workload) *istiov1alpha3.DestinationRule { dr := \u0026amp;istiov1alpha3.DestinationRule{ ObjectMeta: metav1.ObjectMeta{ Name: workload.GetIstioResourceName(), Namespace: workload.GetNamespace(), }, } for _, target := range workload.Spec.Targets { dr.Spec.Subsets = append(dr.Spec.Subsets, \u0026amp;istionetworkingv1alpha3.Subset{ Name: target.TrafficSplit.Subset.Name, Labels: target.TrafficSplit.Subset.Labels, }) } dr.Spec.Host = workload.Spec.Host return dr } Telling the controller to \u0026lsquo;watch\u0026rsquo; the resources we own One last trick left in the implementation of our controller is, when we set the Owner references for each created VirtualService and DestinationRule in these lines:\n1 2 3 4 5 6 ... err = ctrl.SetControllerReference(instance, vs, r.Scheme) ... err := ctrl.SetControllerReference(instance, dr, r.Scheme) ... We are essentially telling kubernetes that these objects are \u0026ldquo;owned\u0026rdquo; by THIS controller\nWe can then ask kubernetes to send is events related to these \u0026ldquo;OWNED\u0026rdquo; resources whenever anything happens with them (delete/update)\nTo do this we go back to the end of the controllers/workload_controller.go and add the following:\n1 2 3 4 5 6 7 func (r *WorkloadReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr). For(\u0026amp;labsv1alpha1.Workload{}). Owns(\u0026amp;istiov1alpha3.VirtualService{}). // we own both these type of objects Owns(\u0026amp;istiov1alpha3.DestinationRule{}). // hence send us events related to these Complete(r) } Are we done? One last thing. Since Istio\u0026rsquo;s VirtualService and DestinationRule are external kubernetes obejcts (for which we have registered the CRD\u0026rsquo;s while installing istio), we are still using raw golang objects / structs and passing it to the controller-runtime client.\nThe client needs some additional information to \u0026ldquo;map the golang obejcts with the registered crds\u0026rdquo;.\nNote that the golang objects we created don\u0026rsquo;t have information like apiVersion, kind Hence to provide this information we need to register something called as a scheme.\nA Scheme basically maps the Go object to what we call GVK(GroupVersionKind) in kubernetes. This is usually auto generated when CRD\u0026rsquo;s are defined using any framework like kubebuilder or operatorsdk. There\u0026rsquo;s even one generated for our Workload kind which you can take a look at here – api/v1alpha1/groupversion_info.go\n1 2 3 4 5 6 7 8 9 10 11 12 ... var ( // GroupVersion is group version used to register these objects GroupVersion = schema.GroupVersion{Group: \u0026#34;labs.ishankhare.dev\u0026#34;, Version: \u0026#34;v1alpha1\u0026#34;} // SchemeBuilder is used to add go types to the GroupVersionKind scheme SchemeBuilder = \u0026amp;scheme.Builder{GroupVersion: GroupVersion} // AddToScheme adds the types in this group-version to the given scheme. AddToScheme = SchemeBuilder.AddToScheme ) A similar scheme is defined by the Istio library, which we can register in our main.go\n1 2 3 4 5 6 7 8 9 10 11 12 import ( ... istioscheme \u0026#34;istio.io/client-go/pkg/clientset/versioned/scheme\u0026#34; ... ) ... func init() { ... _ = istioscheme.AddToScheme(scheme) } Now we are done! You can follow along the Controller-demo to see the magic happen!\nSome final highlights As pointed out the final sections of the demo, even if someone/something accidentally or deliberately deletes either of the VirtualService or the DestinationRule, the controller immediately steps in and recreates it for us.\nOne more trick left to showcase. If you were to go and delete either the VirtualService or the DestinationRule now, since the controller is watching over them, it will instantly recreate it with the correct configuration. This simulates an actual human error which have triggered alerts for the team in the past only to realise that somewhere, somebody did a kubectl delete vs helloworld. Go ahead, try it. Then try to run kubectl get vs,dr again.\nLastly I\u0026rsquo;d like to mention the awesome work being done by folks at @crossplane and @hasheddan who are building some really cool stuff leveraging the controllers and the KRM(kubernetes resource model). If you\u0026rsquo;re more interested, checkout the following resources:\nKubernetes as a Framework for Control Planes. Crossplane team integrating GKE Autopilot before the night ends on live stream. What\u0026rsquo;s next? In a follow-up post I\u0026rsquo;ll talk about how we can bring Admission Webhooks, particularly Validating Admission Webhooks into the picture and make sure nobody\u0026rsquo;s able to create our Workload CRD with wrong values – like a traffic split that doesn\u0026rsquo;t add up to a 100% for eg. (80:30 or 40:30)\nThis post was original posted on my personal blog ishankhare.dev\n","date":"2021-03-02T00:00:00Z","image":"https://ishankhare.dev/p/teaching-kubernetes-service-mesh-tricks/brahmaputra_huc7cb4a692ba86d21dca6ace88275abce_1697843_120x120_fill_q75_box_smart1.jpg","permalink":"https://ishankhare.dev/p/teaching-kubernetes-service-mesh-tricks/","title":"Teaching kubernetes service mesh tricks"},{"content":"How hashmaps work? Okay, so lets start with what hashmaps are? Hashmaps or hashtables are known by different names in different languages.\nPython has dict/dictionaries Ruby calls it Hash Java has hashmap C++ has unordered_map Even Javascript has maps, which is how basically objects are implemented in javascript. (just look at JSON) What the last point says about hashmaps being used to implement objects in Javascript is also true in case of Python, Ruby and a few others.\nSo hashmaps/dicts/hashtables whatever you want to call them are basically key-value storage data structures. You pass in a value identified by a key, and you can get back the same value with that key - simple as that.\nSo lets try and get hands on with how it looks in code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 a = {} # add a key-value pair a[\u0026#39;name\u0026#39;] = \u0026#39;ishan\u0026#39; # lets see whats in there print(a) # {\u0026#39;name\u0026#39;: \u0026#39;ishan\u0026#39;} # add more key-value pairs a[\u0026#39;age\u0026#39;] = 23 a[\u0026#39;gender\u0026#39;] = \u0026#39;male\u0026#39; print(a) # {\u0026#39;gender\u0026#39;: \u0026#39;male\u0026#39;, \u0026#39;age\u0026#39;: 23, \u0026#39;name\u0026#39;: \u0026#39;ishan\u0026#39;} # note above that the keys are not in the same order as we entered them, # this is because python\u0026#39;s dictionaries are unordered # see https://stackoverflow.com/a/15479974/2972348 # get a specific value print(a[\u0026#39;name\u0026#39;]) # \u0026#39;ishan\u0026#39; # let\u0026#39;s update a value a[\u0026#39;age\u0026#39;] = 24 print(a) # {\u0026#39;gender\u0026#39;: \u0026#39;male\u0026#39;, \u0026#39;age\u0026#39;: 24, \u0026#39;name\u0026#39;: \u0026#39;ishan\u0026#39;} # let\u0026#39;s delete some values del a[\u0026#39;age\u0026#39;] print(a) # {\u0026#39;gender\u0026#39;: \u0026#39;male\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;ishan\u0026#39;} That gives us the list of basic operations on a dict:\ninsert fetch delete The same can be shown with Javascript:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 a = {} a[\u0026#39;name\u0026#39;] = \u0026#39;ishan\u0026#39; console.log(a) // { name: \u0026#39;ishan\u0026#39; } // or the other way a.age = 24 console.log(a) // { name: \u0026#39;ishan\u0026#39;, age: 24 } console.log(a.name) // ishan delete a.age console.log(a) // { name: \u0026#39;ishan\u0026#39; } Enough of examples now, lets get to see how these things work on the inside\nSo, we will be implementing a simple hashtable in Golang. If you haven’t heard or written code in Go, and are skeptical about understanding the code that follows - worry not, Go has a syntax very similar to C and Javascript. And if you can code (in a language), you’re gonna do just fine!\nThe building blocks This image taken from wikipedia shows the basic structure of our hashtable. This will help us understand and break down the problem at hand better:\nTo summarize, we have:\nthe hash function the linear array that the hash function maps to and the actual data nodes that hold our key-value pairs The array So lets start with our linear array to begin with. So we basically need a statically allocated array of some size n. This will be the array holding the pointer to our actual key value pairs. This is how our basic array will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 +----+ 1 | | +----+ 2 | | +----+ 3 | | +----+ 4 | | +----+ 5 | | +----+ 6 | | +----+ 7 | | +----+ 8 | | +----+ 9 | | +----+ 10| | +----+ Let us code this up in Go. To have a basic array inside our hashmap data type. This is going to be fairly easy to achieve.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main import \u0026#34;fmt\u0026#34; const MAP_SIZE = 50 type HashMap struct { Data []*int\t} func NewDict() *HashMap { return \u0026amp;HashMap{}\t} func main() { a := NewDict() } The above code is explained below:\nWe declare the current file as main package with package main.\nWe import a package called “fmt”, we’ll later use it to print things to terminal.\nWe declare a constant variable MAP_SIZE with value 50 . This will be the size of our linear array.\nFor the sake of simplicity, we are assuming that we just have a fixed size array that we use for implementing our hashmap, and we don’t take resizing into account. Actual implementations do account for these things - which will be covered in a future blog post. For more info refer here.\nNext we create a function NewDict that creates this array for us and returns a pointer to it.\nThis completes our bare-bones structure for the array itself. But we are presented with the next challenge. Which is, how to represent the key-value pairs that contain our actual data and link it with our array.\nCreating the nodes and linking with array There is one problem in the code presented above — we have created an array of pointer to integer values. Basically the Data []*int on line8. This is wrong because we need to actually not point to integers but to some object that holds our key-value pairs. We don’t know what that object will be yet, but we know that it needs at least 2 fields — key and value. So lets go ahead and create this new object’s structure in our code.\nWe add the following lines:\n1 2 3 4 5 type Node struct { key string value string next *Node } We create a structure to hold our key-value pairs and a pointer to a Node type. We will need this pointer later on in order to deal with hash conflicts. More on this later, for now just let it sit around there.\nNow since we want every index of our array to point to these Node type, we should change the type of our data array.\n1 2 3 type HashMap struct { Data []*Node } So at the end of these changes, this is how our code should look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import \u0026#34;fmt\u0026#34; const MAP_SIZE = 50 type Node struct { key string value string next *Node } type HashMap struct { Data []*Node } func NewDict() *HashMap { return \u0026amp;HashMap{ Data: make([]*Node, MAP_SIZE) } // we initialize our array with make(), see // https://golang.org/pkg/builtin/#make } func main() { a := NewDict() fmt.Println(\u0026#34;%q\u0026#34;, a) } If you run this program right now, you should see something like this:\n1 2 3 4 5 \u0026amp;{[\u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt;]} We have our hashmap object consisting of an array of 50 pointers each pointing to \u0026lt;nil\u0026gt; which is something equivalent to:\nNULL in C. null in Java. None in Python. undefined in Javascript. This is just as expected, we haven’t inserted any value in the hash table so all pointers are pointing to null/nil . Next we have to think about inserting objects into our hashmap.\nBefore we could even reach that, we need to find a way to convert our keys to corresponding indexes on the array.\nA typical implementation of hash maps relies on a good hash function. The job of a hash function is to convert the value passed to it (in our current case, a string) and return an integer value representing an index on the array. And this integer value has to be the same every time.\nThis means that if hash(\u0026quot;hello\u0026quot;) gives me 966170508347869221, then every time I pass \u0026quot;hello\u0026quot; I should get the same 966170508347869221 as a return value.\nYou can test the above mentioned code in the Python interpreter. Python has a builtin function hash that will return the hash value.\nBut we cannot have index 966170508347869221 on our array. Currently we have only indexes 1-49 available. Hence we need to somehow bring it down to this range.\nModulo to the rescue The simplest solution to this problem is using the modulo (%) operator. This operator gives us the remainder of the division performed. That is, if\n966170508347869221 / 50 = 19323410166957384 + 21\nthen 966170508347869221 % 50 = 21. The remainder.\nBut with this, we have another problem, there will be keys which will be mapped to the same index, hence we’ll get a hash collision.\nFrom this point on, we have 3 problems to solve at hand:\nHow do you implement the hash function. Make the hash value fall in range of our array. How to manage hash collisions. Let’s start with the hash generation function first\nChoosing a hash algorithm Wikipedia has a list of many hash_functions. Which you can refer here https://en.wikipedia.org/wiki/List_of_hash_functions\nFor our purpose, we’ll use a Jenkins hash function. It is a hash function that produces 32-bit hashes. The wikipedia article also has the implementation of the algorithm in C. We’ll just translate that to our go code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func hash(key string) (hash uint32) { hash = 0 for _, ch := range key { hash += uint32(ch) hash += hash \u0026lt;\u0026lt; 10 hash ^= hash \u0026gt;\u0026gt; 6 } hash += hash \u0026lt;\u0026lt; 3 hash ^= hash \u0026gt;\u0026gt; 11 hash += hash \u0026lt;\u0026lt; 15 return } The above function uses a named-return — see https://golang.org/doc/effective_go.html#named-results\nWith this in place our code should look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package main import \u0026#34;fmt\u0026#34; const MAP_SIZE = 50 type Node struct { key string value string next *Node } type HashMap struct { Data []*Node } func hash(key string) (hash uint32) { hash = 0 for _, ch := range key { hash += uint32(ch) hash += hash \u0026lt;\u0026lt; 10 hash ^= hash \u0026gt;\u0026gt; 6 } hash += hash \u0026lt;\u0026lt; 3 hash ^= hash \u0026gt;\u0026gt; 11 hash += hash \u0026lt;\u0026lt; 15 return } func NewDict() *HashMap { return \u0026amp;HashMap{ Data: make([]*Node, MAP_SIZE) } // we initialize our array with make(), see // https://golang.org/pkg/builtin/#make } func main() { a := NewDict() fmt.Println(a) } Mapping the hash to our array Now that we have found our way to convert our keys to the hash value, we now need to make sure that this value falls within out array. For this we implement a getIndex function:\n1 2 3 func getIndex(key string) (index int) { return int(hash(key)) % MAP_SIZE } With this in place, we have taken care of 2 of our 3 problem statements. The 3rd problem - managing collisions will be taken care of at a later stage. First we now need to focus on inserting the key-value pairs in our array. Lets write a function for that.\nInsertion The following function uses a receiver, which are a powerful feature of go.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func (h *HashMap) Insert(key string, value string) { index := getIndex(key) if h.Data[index] == nil { // index is empty, go ahead and insert h.Data[index] = \u0026amp;Node{key: key, value: value} } else { // there is a collision, get into linked-list mode starting_node := h.Data[index] for ; starting_node.next != nil; starting_node = starting_node.next { if starting_node.key == key { // the key exists, its a modifying operation starting_node.value = value return } } starting_node.next = \u0026amp;Node{key: key, value: value} } } The code is mostly self explanatory, but we\u0026rsquo;ll briefly go over it anyway.\nWe first call getIndex with the key, which in-turns calls the hash function internally. This gives us the index on our array where we can store that key-value pair.\nNext we check if the index is empty, if yes - then we simply store the K-V Pair there. This would look something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 +----+ 1 | | +----+ 2 | | +----+ +------------------+ 3 | * |----\u0026gt;| Key: Value | +----+ | Next: nil | 4 | | +------------------+ +----+ 5 | | +----+ +------------------+ 6 | * |----\u0026gt;| Key: Value | +----+ | Next: nil | 7 | | +------------------+ +----+ 8 | | +----+ 9 | | +----+ 10| | +----+ The above denotes the if part of our code - when there are no collisions. The else part of our code handles the collision.\nThere are a few different ways of handling collisions in hashmaps:\nSeparate chaining with linked lists Open addressing Double Hashing The current code in the else block handles collisions with the Separate chaining with linked lists technique.\nThe linked list technique will result in a data structure very similar to the following image:\nThe two keys mapping to the same index - hence resulting in a collision are shown in red.\nNext we\u0026rsquo;ll talk about retrieving the value back from our hashtable\nFetch The next basic operation on hashmaps that we are going to look at is Fetch. Fetch is fairly simple and identical to insert - we pass our key as input to the hash function and get a hash value, which is then mapped to our array using the getIndex function.\nWe can then have 3 possible outcomes:\nWe check if the key on that index is matching the we are looking for - if yes, then we have our match. If the key does not match, we check if its next value is not nil - basically checking for collision and find that element in the separate chained linked list If both of the above conditions fail, we concur that the requested key does not exist in our hashtable. The code implementing the above explanation is shown below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func (h *HashMap) Get(key string) (string, bool) { index := getIndex(key) if h.Data[index] != nil { // key is on this index, but might be somewhere in linked list starting_node := h.Data[index] for ; ; starting_node = starting_node.next { if starting_node.key == key { // key matched return starting_node.value, true } if starting_node.next == nil { break } } } // key does not exists return \u0026#34;\u0026#34;, false } The above code uses golang\u0026rsquo;s multiple return values feature.\nWith this in place, we\u0026rsquo;ll write the main function and test these operations that we have implemented.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { a := NewDict() a.Insert(\u0026#34;name\u0026#34;, \u0026#34;ishan\u0026#34;) a.Insert(\u0026#34;gender\u0026#34;, \u0026#34;male\u0026#34;) a.Insert(\u0026#34;city\u0026#34;, \u0026#34;mumbai\u0026#34;) a.Insert(\u0026#34;lastname\u0026#34;, \u0026#34;khare\u0026#34;) if value, ok := a.Get(\u0026#34;name\u0026#34;); ok { fmt.Println(value); } else { fmt.Println(\u0026#34;Value did not match!\u0026#34;) } fmt.Println(a) } Also lets add a convenience function that allows us to print our hashmap in the desired format.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (h *HashMap) String() string { var output bytes.Buffer fmt.Fprintln(\u0026amp;output, \u0026#34;{\u0026#34;) for _, n := range h.Data { if n != nil { fmt.Fprintf(\u0026amp;output, \u0026#34;\\t%s: %s\\n\u0026#34;, n.key, n.value) for node := n.next; node != nil; node = node.next { fmt.Fprintf(\u0026amp;output, \u0026#34;\\t%s: %s\\n\u0026#34;, node.key, node.value) } } } fmt.Fprintln(\u0026amp;output, \u0026#34;}\u0026#34;) return output.String() } This overrides the default print output for our defined HashMap type.\nThe above method is just a convenience method used for printing the entire hashmap in a pretty format. It is in no way required for proper functioning of the data structure\nTo sum it up, here\u0026rsquo;s our final code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;bytes\u0026#34; ) const MAP_SIZE = 10 type Node struct { key string value string next *Node } type HashMap struct { Data []*Node } func NewDict() *HashMap { return \u0026amp;HashMap{ Data: make([]*Node, MAP_SIZE) } } func (n *Node) String() string { return fmt.Sprintf(\u0026#34;\u0026lt;Key: %s, Value: %s\u0026gt;\\n\u0026#34;, n.key, n.value) } func (h *HashMap) String() string { var output bytes.Buffer fmt.Fprintln(\u0026amp;output, \u0026#34;{\u0026#34;) for _, n := range h.Data { if n != nil { fmt.Fprintf(\u0026amp;output, \u0026#34;\\t%s: %s\\n\u0026#34;, n.key, n.value) for node := n.next; node != nil; node = node.next { fmt.Fprintf(\u0026amp;output, \u0026#34;\\t%s: %s\\n\u0026#34;, node.key, node.value) } } } fmt.Fprintln(\u0026amp;output, \u0026#34;}\u0026#34;) return output.String() } func (h *HashMap) Insert(key string, value string) { index := getIndex(key) if h.Data[index] == nil { // index is empty, go ahead and insert h.Data[index] = \u0026amp;Node{key: key, value: value} } else { // there is a collision, get into linked-list mode starting_node := h.Data[index] for ; starting_node.next != nil; starting_node = starting_node.next { if starting_node.key == key { // the key exists, its a modifying operation starting_node.value = value return } } starting_node.next = \u0026amp;Node{key: key, value: value} } } func (h *HashMap) Get(key string) (string, bool) { index := getIndex(key) if h.Data[index] != nil { // key is on this index, but might be somewhere in linked list starting_node := h.Data[index] for ; ; starting_node = starting_node.next { if starting_node.key == key { // key matched return starting_node.value, true } if starting_node.next == nil { break } } } // key does not exists return \u0026#34;\u0026#34;, false } func hash(key string) (hash uint8) { // a jenkins one-at-a-time-hash // refer https://en.wikipedia.org/wiki/Jenkins_hash_function hash = 0 for _, ch := range key { hash += uint8(ch) hash += hash \u0026lt;\u0026lt; 10 hash ^= hash \u0026gt;\u0026gt; 6 } hash += hash \u0026lt;\u0026lt; 3 hash ^= hash \u0026gt;\u0026gt; 11 hash += hash \u0026lt;\u0026lt; 15 return } func getIndex(key string) (index int) { return int(hash(key)) % MAP_SIZE } func main() { a := NewDict() a.Insert(\u0026#34;name\u0026#34;, \u0026#34;ishan\u0026#34;) a.Insert(\u0026#34;gender\u0026#34;, \u0026#34;male\u0026#34;) a.Insert(\u0026#34;city\u0026#34;, \u0026#34;mumbai\u0026#34;) a.Insert(\u0026#34;lastname\u0026#34;, \u0026#34;khare\u0026#34;) if value, ok := a.Get(\u0026#34;name\u0026#34;); ok { fmt.Println(value); } else { fmt.Println(\u0026#34;Value did not match!\u0026#34;) } fmt.Println(a) } Running On running the above code, we get the following output:\n1 2 3 4 5 6 7 ishan { city: mumbai name: ishan lastname: khare gender: male } The first line is the value of our key name. The output from 2nd line onwards is basically our custom-coded pretty-print function that outputs our entire hashmap. If you\u0026rsquo;re interested in running/experimenting with this code, please feel free to fork the following repl. You can also play around on this webpage interactively.\nOur current implementation only supports strings as keys. In the next blog post we\u0026rsquo;ll talk about how we can make this a more generic implementation.\nThis post was originally published on my blog at ishankhare.com\n","date":"2019-03-08T00:00:00Z","image":"https://ishankhare.dev/p/lets-implement-a-hashtable/beach_hubcfc686359850358f8b4a845b0bcb2e4_1032253_120x120_fill_q75_box_smart1.jpg","permalink":"https://ishankhare.dev/p/lets-implement-a-hashtable/","title":"Let's implement a hashtable"}]